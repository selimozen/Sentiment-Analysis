{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "import nltk\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"stock_data.csv\", encoding = \"ISO-8859-1\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.272664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.962192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment\n",
       "count  5791.000000\n",
       "mean      0.272664\n",
       "std       0.962192\n",
       "min      -1.000000\n",
       "25%      -1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"] = data[\"Sentiment\"].replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3685\n",
       "0    2106\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tr - Veri ön işleme için gerekli aletlerimizi tanımlıyoruz ve tüm veri ön işlemeyi bir for döngüsü ile aradan çıkarıyoruz.\n",
    "\n",
    "### Eng - We define our tools for data preprocessing and do all data preprocessing in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "stopwordSet = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = list()\n",
    "for i in range(len(data)):\n",
    "    text = re.sub('[^a-zA-Z]',\" \",data['Text'][i])\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text,language=\"english\")\n",
    "    text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet]\n",
    "    text = \" \".join(text)\n",
    "    text_reviews.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tr - Özellik çıkarımı kısmına geldik. Bu kısımda metinlerimizi temsil eden vektörler oluşturacağız ve modellerimizi bu vektörlerimize göre eğiteceğiz. Modelin başarısını etkileyen önemli faktörlerdendir. Ben 2 farklı yöntemin 2'sinide değerlendirdim ve başarılı olmayanı yorum içine aldım.\n",
    "\n",
    "#### Eng - We're in the feature extraction part. In this section, we will create vectors that represent our texts and train our models according to these vectors. It is one of the important factors affecting the success of the model. I evaluated 2 of the 2 different methods and commented on the unsuccessful one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tr - Aslın başarılı sayabileceğimiz 2 modelimiz var. Count vectorizer sadece lojistic regresyon için başarısızdır.\n",
    "\n",
    "Eng - In fact, we have 2 models that we can consider successful. Count vectorizer fails only for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, encoding='utf-8',\n",
    " decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_extraction.text import CountVectorizer\\nvectorizer = CountVectorizer(binary= False, ngram_range = (1,2))\\n#accurucy maalesef %78'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary= False, ngram_range = (1,2))\n",
    "#accurucy maalesef %78'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(text_reviews)\n",
    "X = vectorizer.transform(text_reviews)\n",
    "y= data['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state = 21, solver = \"saga\", C = 8, penalty = 'l2')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "accuracy: 0.7946505608283002\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tr - GBM Kullanıyoruz.\n",
    "#### Eng - We are using GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.50, max_depth = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tr - Support Vector Machines Kullanıyoruz.\n",
    "\n",
    "#### Eng - We are using Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(C = 5 , kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "accuracy: 0.7963761863675582\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, y_pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tr -  XGBClassifier Kullanıyoruz\n",
    "#### Eng - We are using XGBClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimator = 500, max_depth = 5, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:59:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimator } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.5, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimator=500, n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "accuracy: 0.7679033649698016\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, y_pred4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tr - LighGBM Classifier Kullanıyoruz.\n",
    "#### Eng - L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = y_train.astype('float32')\n",
    "Y_test = y_test.astype('float32')\n",
    "x_train = X_train.astype('float32')\n",
    "x_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(learning_rate = 0.10, n_estimator = 500, max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(max_depth=8, n_estimator=500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.fit(x_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5 = lgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "accuracy: 0.7515099223468508\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\naccuracy: {}\".format(accuracy_score(y_test, y_pred5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonuç: En hızlı ve en yüksek accurcy değerini veren SVM ardından ise lojistik regresyondur. En kötüsü ise yavaşlığından doalyı GBM algoritmasıdır. Ağlattı beni ya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
